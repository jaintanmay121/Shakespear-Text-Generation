{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Shakespeare Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z2xZnxncD2Ep",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFExi_lC2oO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "41b7cc00-d049-4f5a-cb97-4199d34a705f"
      },
      "source": [
        "!wget 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-07 16:50:20--  https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 2404:6800:4008:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-05-07 16:50:20 (88.0 MB/s) - ‘shakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzfX7hK8G0bn",
        "colab": {}
      },
      "source": [
        "text = open('shakespeare.txt').read()\n",
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GqpuKh9HMNnf"
      },
      "source": [
        "## Process the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VPXkB5bOITqi",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))    #Contains every character used"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xFFpuXfGMPq2",
        "colab": {}
      },
      "source": [
        "char2index = {char: index for index, char in enumerate(vocab)}   #character to index\n",
        "index2char = np.array(vocab)        #index to character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DXUAlYmvN_Rj",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array([char2index[char] for char in text])      #Converting the `text` into `int`"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pf5tMMDnUP3q",
        "colab": {}
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)       #Creating tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ap71VjB2Vuct",
        "colab": {}
      },
      "source": [
        "#Creating batches of data\n",
        "char_dataset = char_dataset.batch(100 + 1, drop_remainder=True) #lenght of the each sentence=100 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fxvXsP0XFDh",
        "colab": {}
      },
      "source": [
        "def inp_target(sent):\n",
        "    inp = sent[:-1]\n",
        "    target = sent[1:]\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "454rWIQYXXRY",
        "colab": {}
      },
      "source": [
        "dataset = char_dataset.map(inp_target)   #Prepare input and target data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eDq-wa5EC3wW",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAkLzg4L5XRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.models.Sequential([tf.keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                                  output_dim=embedding_dim,\n",
        "                                                                  batch_input_shape=[batch_size, None]),\n",
        "                                        \n",
        "                                        tf.keras.layers.LSTM(units=rnn_units,\n",
        "                                                             return_sequences=True,\n",
        "                                                             stateful=True,\n",
        "                                                             recurrent_initializer=tf.keras.initializers.GlorotNormal()),\n",
        "                                        \n",
        "                                        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I7ZuvZHBD_pS",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoPwxyAPEg6z",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLnlZFgU55bQ",
        "outputId": "58e86215-e312-4dfe-aaed-9fcff52fda1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SXhJsB6eFgrJ",
        "colab": {}
      },
      "source": [
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LUhXnHPJFy5q",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'checkpoints'\n",
        "os.makedirs('checkpoint_dir', exist_ok=True)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFg9MFJoGZWf"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AVk-pARPGaja",
        "colab": {}
      },
      "source": [
        "EPOCHS=40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y0rveBdAGeEz",
        "outputId": "13368fd5-8233-43f5-a212-5c7ca6c82f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(x=dataset,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 28s 161ms/step - loss: 2.5909\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 28s 162ms/step - loss: 1.8888\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.6382\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.5048\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.4243\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.3675\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.3226\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.2843\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.2482\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.2136\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 28s 164ms/step - loss: 1.1780\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.1425\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.1055\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.0675\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 1.0277\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.9884\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.9475\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.9071\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.8676\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.8298\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.7929\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.7585\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.7267\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.6979\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.6709\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.6450\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.6251\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.6031\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5868\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5689\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5522\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5385\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5286\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5180\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.5055\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.4972\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.4910\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.4809\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.4743\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 28s 163ms/step - loss: 0.4698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X-dhNP2OG2EM"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7evN0LvH01P",
        "colab": {}
      },
      "source": [
        "simplified_batch_size = 1\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([simplified_batch_size, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y3eduDtZI9zQ",
        "outputId": "e603102d-7fc6-482b-f605-dbf0bfbf7986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bOqdqGouJFf_",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    \n",
        "    input_indices = [char2index[s] for s in start_string]\n",
        "    input_indices = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    \n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions,\n",
        "                                             num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z-8e8P60J5Pg",
        "outputId": "1c19aa25-f6e8-4567-e6cd-3c04484fc9c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: farewell; for I was time to be perfidiously;\n",
            "Meaning the tide with you.\n",
            "For I were not weep we are after tow.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "KING EDWARD IV:\n",
            "Clarence and Somerset both grow! a malice,\n",
            "That thieves do so, so that the kingly hath\n",
            "Found his most sovereign? O my well, come for't.\n",
            "\n",
            "PETRICANI:\n",
            "Go, away with her, trust me with roaring more requite; like daughter,\n",
            "Nor more such name in this exgredier.\n",
            "What looks the tribunes of the news I turn me from the mere you:\n",
            "From whence shall I be patient. You, my lords,\n",
            "With thoughts from birth of all my loving father.\n",
            "\n",
            "KING HENRY VI:\n",
            "We does with you; and so he shall never\n",
            "Be calm the Duke of Lancaster letting\n",
            "Than all worm of inecity,\n",
            "Which wither'd his friends, he would desire us:\n",
            "I have seen an inthrone can be too: true, instation\n",
            "That by my faith dread father: Mont go what:\n",
            "I'll not be longer than they rejeight,\n",
            "Which makes thee appear in the midnight\n",
            "For peoplext here with her that shall not prove,\n",
            "And I'll believe me to my nn occasion of my state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wq_NlwWJSdix",
        "outputId": "d328a776-e67a-4df0-fc5b-4eff65d6b554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \", temperature=0.01))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: IIGHAM:\n",
            "Marry, my lord, let me know your miserable.\n",
            "And it concluded the prince, craftress lies\n",
            "Against the words of slaughing men!\n",
            "\n",
            "Second Servant:\n",
            "O, this is it that makes this spoil were frown.\n",
            "\n",
            "LUCENTIO:\n",
            "I pray you are to make thee speak.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Many years of happy son, he stands by what thou wert possess'd,\n",
            "Which art possess'd now to depose thyself.\n",
            "Why, cousin, I'll keep the prince, and call thee back\n",
            "With twenty hundred thousand times before the water.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, what is that to me that?\n",
            "\n",
            "GLOUCESTER:\n",
            "I cannot blame her: by God's holy mother,\n",
            "She hath had more for fathers you shall be the man.\n",
            "But stay the triumph of great Bolingbroke?\n",
            "Gardener, for telling me these news of his transported\n",
            "and hanging of a king: beseech you, and my knightly time\n",
            "To say 'What is it, Buckingham?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Marry, my lord, lett she shall bring forth my heart,\n",
            "Which is the bell: so sighs and tears and groans\n",
            "Show minutes, time, go with me;\n",
            "I charge thee in the victory of all the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0hh80MqEO_XI"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VPE98xa8PA-u",
        "colab": {}
      },
      "source": [
        "model_name = 'shakespeare_text_generation.h5'\n",
        "model.save(model_name, save_format='h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrQsr1Iv7v9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}